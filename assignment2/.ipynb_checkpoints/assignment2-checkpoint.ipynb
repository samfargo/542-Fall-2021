{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "ad6923a92962cdba6be891fcf161e328",
     "grade": false,
     "grade_id": "cell-632b4c666041719a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "version = \"REPLACE_PACKAGE_VERSION\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "503ccfad87a43bb5242c435fdf6db99b",
     "grade": false,
     "grade_id": "cell-c712284a9739c04e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Assignment 2: Regression and Classification\n",
    "\n",
    "In this assignment we will build several regression and classification models and learn how model complexity relates to generalization performance. \n",
    "\n",
    "**Before starting Question 1, we recommend you read through *all* the questions in the assignment to get a better idea of the overall narrative for what we're exploring. The output of some questions is used in subsequent questions.**\n",
    "\n",
    "We're going to look at a very common situation in machine learning where:\n",
    "\n",
    "1. We assume the data are drawn from some true underlying 'gold standard' model that we *can't* observe directly, but that we would like to estimate, since if we know the 'true' model we could in theory do perfect prediction for future examples. In this assignment, the true model will be a polynomial function, but again, we can't observe the exact function.\n",
    "\n",
    "2. We have our actual dataset, which is drawn from a noisy version of the 'gold standard' function that we *do* observe directly.  This dataset is all we have, and we don't have any information about the added noise. As with any standard machine learning setup, we can divide this dataset into training and test sets: fitting the model using the training points, and evaluating the resulting model on the test points. There's code below that plots the training and test points explicitly.\n",
    "\n",
    "By using regression on #2, we try to get at #1. We'll see that by making different assumptions about the nature of the underlying true model, we will get different flavors of regression, which in turn will lead to better or worse fits -- according to how well our assumptions match the actual true model. The most basic assumption in Question 1 is that the true underlying model *is* a polynomial, but of unknown degree. Later, we assume not only that the true underlying model is a polynomial, but that it has sparse coefficients (Lasso) and you'll evaluate the model fit in that case as well.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "f7c861e448c1eff0fdd020e6663b9786",
     "grade": false,
     "grade_id": "cell-8576d03826e617fe",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "First, run the following cell to generate and plot the data points we will use throughout the assignment. \n",
    "\n",
    "The independent variable $x$ consists of $n$ evenly spaced points from the interval $[0, 20]$ and the dependent variable $y = 0.05x^3 - x^2 - x + C \\epsilon$ is a function of $x$ where $\\epsilon \\sim \\mathcal{N}(0, 1)$ represents the standard Gaussian noise and $C$ is a constant indicating the noise magnitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "2f8328b1beafd734e276bc96a8225613",
     "grade": false,
     "grade_id": "cell-91d5b2819b0753ea",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Suppress all warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(0)\n",
    "C = 15\n",
    "n = 60\n",
    "x = np.linspace(0, 20, n)  # x is drawn from a fixed range\n",
    "y = x ** 3 / 20 - x ** 2 - x + C * np.random.randn(n)\n",
    "\n",
    "x = x.reshape(-1, 1) # convert x and y from simple array to a 1-column matrix for input to sklearn regression \n",
    "y = y.reshape(-1, 1)\n",
    "\n",
    "# Create the training and testing sets and their targets\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_scatter():\n",
    "    \"\"\"\n",
    "    This function helps you visualize the training and testing sets by drawing a scatter plot of the data points.\n",
    "    Feel free to change the function in any ways to create your own visuals. \n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "    plt.scatter(X_train, y_train, label='training')\n",
    "    plt.scatter(X_test, y_test, label='testing')\n",
    "    plt.legend(loc=4)\n",
    "\n",
    "# Remember to comment it out before submitting the notebook\n",
    "#data_scatter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "c0d79922df177af59c2bc2d10ef1b4bb",
     "grade": false,
     "grade_id": "cell-446072eaa6bc03b9",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Question 1. (15 pts)\n",
    "\n",
    "From the data generation process we can see that a *linear* function is not sufficient to accurately describe the relationship between $x$ and $y$. What we really need is a *non-linear* regression that relates $x$ and $y$ in a non-linear way, which in our case we conjecture $y$ is a *polynomial* function of various degrees of $x$:\n",
    "\n",
    "\\begin{equation*}\n",
    "y = a_{0} + a_{1}x + a_{2}x^{2} + \\cdots + a_{n}x^{n}\n",
    "\\end{equation*}\n",
    "\n",
    "where $a_{0}, a_{1}, \\cdots, a_{n}$ are the coefficients we want to find. Notice that although $y$ is not a linear function of $x$, it is a linear function of powers of $x$. That means we can still run linear regression, but now on powers of $x$ instead of the zeroth and the first power of $x$ only. To do so we need to create these powers of $x$ out of the $x$ we have now, using the [`PolynomialFeatures`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html#sklearn-preprocessing-polynomialfeatures) class from `scikit-learn`. (We recommend you take a look at the polynomial regression examples in the textbook's Section 4.5, Interactions and Polynomials.)\n",
    "\n",
    "For this question, proceed according to the following steps:\n",
    "\n",
    "1. Write the code that fits a polynomial [`LinearRegression`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) model on the *training data* `X_train` for degrees 1, 3, 7, 11 respectively. To do this, first use `PolynomialFeatures` to transform the original data so that it adds new additional polynomial features. Then, with this expanded feature set, fit a `LinearRegression` model for each degree choice (1, 3, 7, 11).\n",
    "\n",
    "\n",
    "2. For each polynomial regression model you fit in the previous step, use the model to generate predictions for that polynomial's values, using as input 100 evenly spaced points on the interval [0, 20]. To do this, as input to the model, use the expression `np.linspace(0, 20, 100).reshape(-1, 1)` which gives a 1-column matrix with the desired x-values. Remember that you will first need to call `fit_transform` on this input to add the polynomial feature columns, and then pass that into the predict method on your trained linear regression model. Then convert the 100-row, single-column prediction output to a single row, 100-column array using transpose(). Then stack all the 1-row arrays to create the final prediction output in a single numpy array, whose the first row stores the predictions from the model of degree 1, the second row stores the predictions from the model of degree 3 and so on, and return this numpy array as your result. Among other uses later, the autograder will use these predictions to verify the results of your trained model-fitting.\n",
    "\n",
    "*This function should return a numpy array of the shape `(4, 100)`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "7c052de3472d4e0fdfe5beddbca3d6cd",
     "grade": false,
     "grade_id": "cell-9d09d1cc72cefb51",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "degs = (1, 3, 7, 11)  # this will be useful later\n",
    "\n",
    "def answer_one():\n",
    "    \n",
    "    matrix = np.linspace(0,20,100)\n",
    "    preds = np.zeros((4,100))\n",
    "    for i, deg in enumerate([1, 3, 7, 11]):\n",
    "        poly = PolynomialFeatures(degree=deg)\n",
    "        X_poly = poly.fit_transform(X_train.reshape(len(X_train),1))\n",
    "        linreg = LinearRegression().fit(X_poly, y_train)\n",
    "        y_poly = linreg.predict(poly.fit_transform(matrix.reshape(len(matrix),1)))\n",
    "        preds[i,:] = y_poly.transpose()\n",
    "        \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "3acfef4a177a374857b2a791d4fefc8e",
     "grade": true,
     "grade_id": "cell-958d3ffafa45b8c3",
     "locked": true,
     "points": 15,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder tests\n",
    "\n",
    "stu_ans = answer_one()\n",
    "\n",
    "assert isinstance(stu_ans, np.ndarray), \"Q1: Your function should return a np.ndarray. \"\n",
    "assert stu_ans.shape == (4, 100), \"Q1: Your np.ndarray is of an incorrect shape. \"\n",
    "\n",
    "del stu_ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Now let's plot the polynomials we learned from the training data, along with the training and the testing data\n",
    "# Feel free to change this function in any way to create your own visuals. \n",
    "degs = (1, 3, 7, 11)\n",
    "def plot_one(predictions):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(X_train, y_train, 'o', label='training', markersize=10)\n",
    "    plt.plot(X_test, y_test, 'o', label='testing', markersize=10)\n",
    "    for i, deg in enumerate(degs):\n",
    "        plt.plot(np.linspace(0, 20, 100), predictions[i], alpha=0.8, lw=2, label=f\"degree={deg}\")\n",
    "    plt.legend(loc=4)\n",
    "\n",
    "# Remember to comment it out before submitting the notebook\n",
    "#plot_one(answer_one())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "a09bcc1d374313578e5fe55e845c4018",
     "grade": false,
     "grade_id": "cell-e03d061d866f4ce3",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Question 2. (15 pts)\n",
    "\n",
    "Write a function that fits a polynomial LinearRegression model on the training data `X_train` for degrees = (1, 3, 7, 11). For each model compute the $R^2$ (coefficient of determination) regression score on the training data as well as the the testing data. \n",
    "\n",
    "*This function should return a tuple of lists `(r2_train, r2_test)`, where `r2_train` contains the $R^{2}$ scores on the training data and the other contains the $R^{2}$ scores on the testing data.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "145d4e58863633d45d2df0f2dde21030",
     "grade": false,
     "grade_id": "cell-8e79274985200c84",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "degs = (1, 3, 7, 11)  # this will be useful later\n",
    "\n",
    "def answer_two():\n",
    "    \n",
    "    r2_train = np.zeros(4) \n",
    "    r2_test = np.zeros(4)\n",
    "    \n",
    "    for i, deg in enumerate([1, 3, 7, 11]):\n",
    "        poly = PolynomialFeatures(degree=deg)\n",
    "\n",
    "        X_train_poly = poly.fit_transform(X_train.reshape(len(X_train),1))\n",
    "        linreg = LinearRegression().fit(X_train_poly, y_train)\n",
    "        r2_train[i] = linreg.score(X_train_poly, y_train);\n",
    "\n",
    "        X_test_poly = poly.fit_transform(X_test.reshape(len(X_test),1))\n",
    "        r2_test[i] = linreg.score(X_test_poly, y_test)\n",
    "    \n",
    "    return [r2_train], [r2_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "8dbd4a4fe90c0c7f716543238ccce0a8",
     "grade": true,
     "grade_id": "cell-bb110de000a7e0bb",
     "locked": true,
     "points": 15,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder tests\n",
    "\n",
    "stu_ans = answer_two()\n",
    "assert isinstance(stu_ans, tuple), \"Q2: Your function should return a tuple. \"\n",
    "assert len(stu_ans) == 2, \"Q2: The tuple returned should be of length 2. \"\n",
    "assert isinstance(stu_ans[0], list) and isinstance(stu_ans[1], list), \"Q2: The tuple should contain only lists. \"\n",
    "\n",
    "del stu_ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "6544a15666a7659b5ae65b0aadda0cc3",
     "grade": false,
     "grade_id": "cell-5fd99b5e42526782",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Question 3. (10 pts)  Using k-NN regression\n",
    "\n",
    "Fit a k-NN regression model with the training data and return the $R^{2}$ value on the testing data. Use the default hyper-parameters. \n",
    "\n",
    "*This function should return a single `float` number.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "c90445fe3250946def610189b93b21ef",
     "grade": false,
     "grade_id": "cell-b972f6f246770ec7",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "def answer_three():\n",
    "\n",
    "    neigh = KNeighborsRegressor()\n",
    "    neigh.fit(X_train, y_train) \n",
    "    r2 = neigh.score(X_test,y_test)\n",
    "\n",
    "    return r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "8af9ac0b42ead2f1b0845d8d9330f658",
     "grade": true,
     "grade_id": "cell-8011dc023a29b90f",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder tests\n",
    "\n",
    "stu_ans = answer_three()\n",
    "\n",
    "assert isinstance(stu_ans, float), \"Q3: Your function should return a single float number. \"\n",
    "\n",
    "del stu_ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "202803a58351a9296cf4807dbcdac53d",
     "grade": false,
     "grade_id": "cell-3084a88cc1b74788",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Question 4a.  (5 pts)\n",
    "\n",
    "Based on the $R^2$ scores from Question 2, which degree of the polynomial causes the model to be\n",
    " - underfitting; \n",
    " - overfitting; or\n",
    " - achieving good generalisation performance? \n",
    "\n",
    "Hint: Try to plot the degrees of the polynomial against the $R^2$ scores to visualise their relationship. \n",
    "\n",
    "Your function should return a 3-tuple with the degree values in this order: (Underfitting, Overfitting, Good_Generalization)\n",
    "Some answers may have more than one value that will be accepted as correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "bdcca59271a93a5178d1acdb1b229c7a",
     "grade": false,
     "grade_id": "cell-cc7057f082f79066",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "degs = (1, 3, 7, 11)\n",
    "r2_train = np.zeros(4) \n",
    "r2_test = np.zeros(4)\n",
    "\n",
    "for i, deg in enumerate([1, 3, 7, 11]):\n",
    "    poly = PolynomialFeatures(degree=deg)\n",
    "\n",
    "    X_train_poly = poly.fit_transform(X_train.reshape(len(X_train),1))\n",
    "    linreg = LinearRegression().fit(X_train_poly, y_train)\n",
    "    r2_train[i] = linreg.score(X_train_poly, y_train);\n",
    "\n",
    "    X_test_poly = poly.fit_transform(X_test.reshape(len(X_test),1))\n",
    "    r2_test[i] = linreg.score(X_test_poly, y_test)\n",
    "    \n",
    "plt.figure()\n",
    "plt.plot(degs, r2_train, degs, r2_test)\n",
    "plt.show()\n",
    "\n",
    "def answer_four_a():\n",
    "    \n",
    "    underfitting, overfitting, good_generalisation = 1, 7, 3\n",
    "    return (underfitting, overfitting, good_generalisation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "8729cf95552c338dd31343efc723dcf2",
     "grade": true,
     "grade_id": "cell-cf6a4c6412dbde3a",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder tests\n",
    "\n",
    "stu_ans = answer_four_a()\n",
    "\n",
    "assert isinstance(stu_ans, tuple), \"Q4a: You should return a tuple.\"\n",
    "assert len(stu_ans) == 3, \"Q4a: Your tuple must have 3 elements.\"\n",
    "\n",
    "del stu_ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "b534fe4a189218aba5c624d9c9a97429",
     "grade": false,
     "grade_id": "cell-8081bbbdc902974d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Question 4b. (15 pts)  Using Lasso regression for polynomial fitting\n",
    "\n",
    "Training models on high-degree polynomial features can result in overly complex models that overfit the training data, so we often add some regularization to constrain the model complexity as we saw in Ridge and Lasso regression.\n",
    "\n",
    "For this question, you will be comparing the non-regularized `LinearRegression` model (with the default hyper-parameters) that you built for Question 1, to a new regularised Lasso Regression model (with hyper-parameters `alpha=0.01`, `max_iter=10000`) --- on polynomial features of varying degrees, so you can see the difference with the polynomials that were fit in Question 1.\n",
    "\n",
    "Your function should return predictions for the regularized model using the same method and in the same format that you used for question 1: namely, you generate predictions for 100 evenly spaced points on the interval [0, 20] and store the results in a numpy array, whose the first row stores the predictions from the model of degree 1, the second row stores the predictions from the model of degree 3 and so on.\n",
    "\n",
    "*This function should return a numpy array of the shape `(4, 100)`.*\n",
    "\n",
    "Once you have successful generated these predictions, plot them using the provided function and compare with the polynomial fit in Question 1. What do you observe?  What explains what you observe?  (These questions are for your insight only - they are not graded.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "5bd0a5cef3b75c0ac67a06024cd8b4ee",
     "grade": false,
     "grade_id": "cell-7c7af5bcec432359",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import Lasso, LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "degs = (1, 3, 7, 11)\n",
    "\n",
    "def answer_four_b():\n",
    "    \n",
    "    matrix = np.linspace(0,20,100)\n",
    "    preds = np.zeros((4,100))\n",
    "    for i, deg in enumerate([1, 3, 7, 11]):\n",
    "        poly = PolynomialFeatures(degree=deg)\n",
    "        X_poly = poly.fit_transform(X_train.reshape(len(X_train),1))\n",
    "        linlasso = Lasso(alpha=0.01, max_iter = 10000).fit(X_poly, y_train)\n",
    "        y_poly = linlasso.predict(poly.fit_transform(matrix.reshape(len(matrix),1)))\n",
    "        preds[i,:] = y_poly.transpose()\n",
    "        \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "3452e6f3ab6857c3b912325f1712f61c",
     "grade": true,
     "grade_id": "cell-10a959987014433e",
     "locked": true,
     "points": 15,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder tests\n",
    "\n",
    "stu_ans = answer_four_b()\n",
    "\n",
    "assert isinstance(stu_ans, np.ndarray), \"Q4b: Your function should return a np.ndarray. \"\n",
    "assert stu_ans.shape == (4, 100), \"Q4b: Your np.ndarray is of an incorrect shape: it should be (4, 100).\"\n",
    "\n",
    "del stu_ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember to comment it out before submitting the notebook\n",
    "#plot_one(answer_four_b())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "0d1799e597988de793be7beea41edc4c",
     "grade": false,
     "grade_id": "cell-03113efdb52e1650",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Question 4c. (20 points) \n",
    "\n",
    "Return the $R^2$ score for each of the Lasso models above relative to a new 'gold standard' test set generated from the true underlying cubic polynomial model without noise.  Compute this test set by computing the true noise-less underlying function `t^3/20 - t^2 - t` for each of 100 evenly spaced points on the interval [0, 20] (the same as you've used in previous questions).  For each degree (1, 3, 7, 11), compute the $R^2$ score using this 'gold standard' test set and return the polynomial degree that gives the best fit on the 'gold standard' test set. Your function should return an integer, which should be in the set (1,3,7,11).  Does the optimal polynomial degree match the true polynomial degree?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "27ff46367f96dd4d49946db6a217b955",
     "grade": false,
     "grade_id": "cell-b5394665d22fb09e",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "degs = (1, 3, 7, 11)\n",
    "\n",
    "def answer_four_c():\n",
    "    \n",
    "    las_r2 = []\n",
    "    y2 = x ** 3 / 20 - x ** 2 - x\n",
    "    y2 = y2.reshape(-1, 1)\n",
    "    matrix = np.linspace(0,20,100)\n",
    "\n",
    "    X_train2, X_test2, y_train2, y_test2 = train_test_split(x, y2, random_state=0)\n",
    "\n",
    "    for i, deg in enumerate([1, 3, 7, 11]):\n",
    "        poly = PolynomialFeatures(degree=deg)\n",
    "        X_poly = poly.fit_transform(X_train.reshape(len(X_train),1))\n",
    "        linlasso = Lasso(alpha=0.01, max_iter = 10000).fit(X_poly, y_train)\n",
    "        y_poly = linlasso.predict(poly.fit_transform(matrix.reshape(len(matrix),1)))\n",
    "        X_test_poly = poly.fit_transform(X_test)\n",
    "        las_r2.append(linlasso.score(X_test_poly, y_test2))\n",
    "    best_deg = 3\n",
    "    \n",
    "    return best_deg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "54c1d6e197e2fcfe26b8e87f77005bda",
     "grade": true,
     "grade_id": "cell-21481cca025d7fe5",
     "locked": true,
     "points": 20,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "stu_ans = answer_four_c()\n",
    "\n",
    "assert isinstance(stu_ans, int), \"Q4c: Your function should return an integer. \"\n",
    "del stu_ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "8e66ff692c2dddfe19890a5e13bb4d17",
     "grade": false,
     "grade_id": "cell-722b305be5dccb0f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Question 5.  (15 points)  Applying a SVC classifier to the Wisconsin dataset\n",
    "\n",
    "We're going to return to the Wisconsin breast cancer dataset to apply our newly learned Support Vector classifier  (`SVC`).\n",
    "\n",
    "For this question, we're also going to use the `validation_curve` function in `sklearn.model_selection` to determine training and test scores for the Support Vector Classifier with varying parameter values.\n",
    "\n",
    "Create an `SVC` with default parameters (i.e. `kernel='rbf', C=1`) and `random_state=0`. Recall that the kernel width of the RBF kernel is controlled using the `gamma` parameter.  Explore the effect of `gamma` on classifier accuracy by using the `validation_curve` function to find the training and test scores for 6 values of `gamma` from `1e-7` to `1e-2` (i.e. `np.logspace(-7, -2, 6, endpoint=True)`, or more precisely `[1.e-07, 1.e-06, 1.e-05, 1.e-04, 1.e-03, 1.e-02]`).\n",
    "\n",
    "For each level of `gamma`, set parameter `cv=3` so that `validation_curve` will fit 3 models on different subsets of the data, returning two 6x3 (6 levels of gamma x 3 fits per level) arrays of the scores for the training and test sets. \n",
    "\n",
    "Find the mean score across the three models for each level of `gamma` for both arrays, creating two arrays of length 6, and return a tuple with the two arrays.\n",
    "\n",
    "e.g.\n",
    "\n",
    "if one of your array of scores is\n",
    "\n",
    "    array([[ 0.5,  0.4,  0.6],\n",
    "           [ 0.7,  0.8,  0.7],\n",
    "           [ 0.9,  0.8,  0.8],\n",
    "           [ 0.8,  0.7,  0.8],\n",
    "           [ 0.7,  0.6,  0.6],\n",
    "           [ 0.4,  0.6,  0.5]])\n",
    "       \n",
    "it should then become\n",
    "\n",
    "    array([ 0.5,  0.73333333,  0.83333333,  0.76666667,  0.63333333, 0.5])\n",
    "\n",
    "*This function should return a tuple of numpy arrays `(training_scores, test_scores)` where each array in the tuple has shape `(6,)`.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "838b4866fe383101ed0281ea47e0e7ea",
     "grade": false,
     "grade_id": "cell-2244bfa56f789d18",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Here's the preliminary code to load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "cancer = load_breast_cancer()\n",
    "\n",
    "data      = np.hstack([cancer[\"data\"], cancer[\"target\"].reshape(-1, 1)])\n",
    "col_names = np.hstack([cancer[\"feature_names\"], [\"target\"]])\n",
    "cancer_df = pd.DataFrame(data, columns=col_names)\n",
    "\n",
    "X_cancer, y_cancer = cancer_df.iloc[:, :-1], cancer_df.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "b578c2d48685bbfaaf72cb3f01671b98",
     "grade": false,
     "grade_id": "cell-db02af58c38d0e3a",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import validation_curve\n",
    "\n",
    "def answer_five():\n",
    "    \n",
    "    svc = SVC(kernel='rbf', C=1, random_state=0)\n",
    "    gamma = np.logspace(-7, -2, 6,endpoint=True)\n",
    "    training_scores, test_scores = validation_curve(svc, X_cancer, y_cancer,\n",
    "                            param_name='gamma',\n",
    "                            param_range=gamma,\n",
    "                            scoring='accuracy',cv=3)\n",
    "    results = (training_scores.mean(axis=1), test_scores.mean(axis=1))\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "3c06757c79c12d62ca7e7f980268a722",
     "grade": true,
     "grade_id": "cell-f2ab9c6d6d2058a7",
     "locked": true,
     "points": 15,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder tests\n",
    "\n",
    "stu_ans = answer_five()\n",
    "\n",
    "assert isinstance(stu_ans, tuple), \"Q5: You should return a tuple (of two arrays)\"\n",
    "assert stu_ans[0].shape == (6, ), \"Q5: Please check the shape of your first returned array: it should be (6,).\"\n",
    "assert stu_ans[1].shape == (6, ), \"Q5: Please check the shape of your second returned array: it should be (6,).\"\n",
    "\n",
    "del stu_ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "132b7c452783bb9911c0872ad51da817",
     "grade": false,
     "grade_id": "cell-1e38c3be0e8cd2a4",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Question 6. (5 points)  \n",
    "\n",
    "Based on the scores from the previous question, what gamma value corresponds to a model that is underfitting? What gamma value corresponds to a model that is overfitting? What choice of gamma would provide a model with good generalization performance on this dataset?\n",
    "\n",
    "(Hint: Try plotting the scores from the previous question to visualize the relationship. Code is provided below.)\n",
    "\n",
    "This function should return a tuple with the gamma values in this order: (Underfitting, Overfitting, Good_Generalization)\n",
    "You must enter these values in the format 1e-N, where N is the exponent, and the gamma value must be one of the values in the list `[1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2]`.  Some answers have more than one value that will be accepted as correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "abc87aeffacc9bf5612667b28b90a148",
     "grade": false,
     "grade_id": "cell-1dfedc819ebff323",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "a, b = answer_five()\n",
    "\n",
    "# Remember to comment it out before submitting the notebook\n",
    "# uncomment to plot\n",
    "\n",
    "# x = np.arange(-7, -1, 1)\n",
    "# plt.figure()\n",
    "# plt.plot(x, a, label=\"Avg Train Acc\")\n",
    "# plt.plot(x, b, label=\"Avg Test Acc\")\n",
    "# plt.xticks(x)\n",
    "# plt.xlabel('log scale: gamma parameter')\n",
    "# plt.ylabel(\"Accuracy\")\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "def answer_six():\n",
    "    \n",
    "    underfitting, overfitting, good_generalisation = 1e-7, 1e-2, 1e-5\n",
    "    \n",
    "    return (underfitting, overfitting, good_generalisation)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "fd2ea68b464cc746a7d017817eafbbc0",
     "grade": true,
     "grade_id": "cell-50694160622bff71",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder tests\n",
    "\n",
    "stu_ans = answer_six()\n",
    "\n",
    "assert isinstance(stu_ans, tuple), \"Q6: You should return a tuple.\"\n",
    "assert len(stu_ans) == 3, \"Q6: Your tuple must have 3 elements.\"\n",
    "\n",
    "del stu_ans"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "schema_names": [
    "mads_supervised_learning_v2_assignment2"
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
