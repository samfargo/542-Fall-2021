{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "3113717b98b050bbad62e84b5c4053e4",
     "grade": false,
     "grade_id": "cell-88c93c4d54fd060c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "version = \"REPLACE_PACKAGE_VERSION\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "8ee5362b51ca81057eb68eb06253688e",
     "grade": false,
     "grade_id": "cell-554a4072c9e433c3",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Assignment 3: Classification and Evaluation\n",
    "\n",
    "In this assignment we will build several classification models and calculate some useful metrics for gauging the performance of these models. The scenario we'll address is \"spam\" e-mail detection: a very important, widely-used supervised machine learning task that attempts to find unsolicited, mass-produced messages that have irrelevant and/or inappropriate content (often mass marketing or attempts at fraud). These are sometimes called \"spam filters\".\n",
    "\n",
    "We treat this task as a binary classification problem: detecting if an email is \"spam\" (Class == 1) or not (Class == 0, a regular/good e-mail). Email systems will typically automatically move messages detected as \"spam\" to a \"Spam\" or \"Deleted\" folder so the user will not have to read them in their regular inbox.\n",
    "\n",
    "In this setup, a *false positive* would mark a regular/good e-mail as spam. The key aspect of the \"spam\" scenario is that false positives are obviously very undesirable, because these would cause people to potentially lose valuable \"good\" messages. So we want a highly precise spam filter that has few/no false positives, but as we'll see, as a consequence it may let more spam through the filter. This is a classic precision / recall tradeoff, which you'll investigate below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "dd939a71bad024b0a3463085aab9f523",
     "grade": false,
     "grade_id": "cell-ba1334eed4710455",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Suppress all warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "4dbbbcde207142f514f0ef412d405bc2",
     "grade": false,
     "grade_id": "cell-9771b1df168291c3",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Question 1. (10 pts)\n",
    "\n",
    "Import the data from `assets/spam.csv`. What is the ratio of the counts of regular (class == 0) to spam (Class == 1) observations in the entire dataset? \n",
    "\n",
    "*This function should return a positive float less than 10.* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "d6b70075a7316e62d1d7a859f988cd9d",
     "grade": false,
     "grade_id": "cell-eb9dacc32d9f3f86",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def answer_one():\n",
    "    \n",
    "    df = pd.read_csv('assets/spam.csv')\n",
    "    frac = len(df[df['Class'] == 0]) / len(df[df['Class'] == 1])\n",
    "    \n",
    "    return frac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "012c714ad852d7ec95589a245fdb8b14",
     "grade": true,
     "grade_id": "cell-9242576998a6487f",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder tests\n",
    "\n",
    "stu_ans = answer_one()\n",
    "assert isinstance(stu_ans, float), \"Q1: Your function should return a float. \"\n",
    "assert 0.0 <= stu_ans <= 10.0, \"Q1: Your answer must be between 0 and 10. \"\n",
    "\n",
    "del stu_ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e0da93d60ece6afc55ff3773290aedce",
     "grade": false,
     "grade_id": "cell-5431ef2fd6068eb9",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Now prepare the data: we break into training and testing sets as usual. But there's another critical step, since we're going to be using *regularized* classification methods on this data. We must first *perform feature normalization so that all features are on a standardized scale.*  We do this by applying the StandardScalar class from sklearn.preprocessing.\n",
    "\n",
    "The details of how we do this are important. You must *first* split the data into training and test sets, and *only after the split*, do the feature normalization. This is to avoid giving information about the range of variables in the test split to the training split, which would be a form of data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "ad93714235ba9a5601b0b46418cee8be",
     "grade": false,
     "grade_id": "cell-341b4f8e1caa6a50",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "df = pd.read_csv(\"assets/spam.csv\")\n",
    "\n",
    "X = df.iloc[:, :-1]\n",
    "y = df.iloc[:, -1]\n",
    "\n",
    "\n",
    "X_train_raw, X_test_raw, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "scaler = StandardScaler().fit(X_train_raw)\n",
    "X_train = scaler.transform(X_train_raw)\n",
    "X_test = scaler.transform(X_test_raw)\n",
    "\n",
    "# Use X_train, X_test, y_train, y_test for all of the following questions.\n",
    "# Also, X_train_raw and X_test_raw will be useful for Question 7."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "c2470a923a2ae89817d809bf8bd9dd84",
     "grade": false,
     "grade_id": "cell-ecf05e08626fd65b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Question 2. (15 pts)\n",
    "\n",
    "We've seen that so-called *dummy* classifiers can be used as a simple *sanity-check* baseline against which to compare real classifier performance. If your classifier can't do much better than the dummy classifier, you probably have more work to do.\n",
    "\n",
    "Using `X_train`, `X_test`, `y_train`, and `y_test`, train two dummy classifiers: (A) one that respects the training set's label distribution and (B) one that classifies everything as the majority class of the training data. Where appropriate, make sure to set the *random_state* parameter to zero. \n",
    "\n",
    "Then on the test set, for each of the classifiers A and B, compute precision, recall, and accuracy. Report your results as a single tuple as shown below. Once you have the results, it's instructive to compare these two different types of dummy baselines to understand why they are different.\n",
    "\n",
    "*This function should a return a tuple of six floats, like so:*\n",
    "\n",
    "*`(precision_score_A, recall_score_A, accuracy_score_A, precision_score_B, recall_score_B, accuracy_score_B)`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "114674cc38dcb2cfb343d55cc75437f8",
     "grade": false,
     "grade_id": "cell-c86283c4010030e7",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def answer_two():\n",
    "    \n",
    "    dummy_strat = DummyClassifier(strategy = 'stratified', random_state=0).fit(X_train, y_train)\n",
    "    dummy_mf = DummyClassifier(strategy = 'most_frequent', random_state=0).fit(X_train, y_train)\n",
    "    \n",
    "    y_pred_mf = dummy_mf.predict(X_test)\n",
    "    y_pred_strat = dummy_strat.predict(X_test)\n",
    "    \n",
    "    precision_score_A = precision_score(y_test, y_pred_strat)\n",
    "    recall_score_A = recall_score(y_test, y_pred_strat)\n",
    "    accuracy_score_A = accuracy_score(y_test, y_pred_strat)\n",
    "    \n",
    "    precision_score_B = precision_score(y_test, y_pred_mf)\n",
    "    recall_score_B = recall_score(y_test, y_pred_mf)\n",
    "    accuracy_score_B = accuracy_score(y_test, y_pred_mf)\n",
    "    \n",
    "    preA, recA, accA, preB, recB, accB = precision_score_A, recall_score_A, accuracy_score_A, precision_score_B, recall_score_B, accuracy_score_B\n",
    "    \n",
    "    return preA, recA, accA, preB, recB, accB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "cd0fa9d136511db5e31fe1543d507ef8",
     "grade": true,
     "grade_id": "cell-bf16d0fdf5486eb0",
     "locked": true,
     "points": 15,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder tests\n",
    "\n",
    "stu_ans = answer_two()\n",
    "\n",
    "assert isinstance(stu_ans, tuple), \"Q2: Your function should return a tuple. \"\n",
    "assert len(stu_ans) == 6, \"Q2: The length of your returned tuple should be 6. \"\n",
    "assert all([isinstance(item, float) for item in stu_ans]), \"Q2: Your tuple should only contain floats. \"\n",
    "#assert isinstance(stu_ans[0], float) and isinstance(stu_ans[1], float) and isinstance(stu_ans[2], float) and isinstance(stu_ans[3], float) and isinstance(stu_ans[4], float) and isinstance(stu_ans[5], float), \"Q2: Your tuple should only contain floats. \"\n",
    "\n",
    "del stu_ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "d5dfd3ce25650b3324d86997018e7a65",
     "grade": false,
     "grade_id": "cell-09786aeeb7272bfe",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Question 3. (15 pts)\n",
    "\n",
    "Using `X_train`, `X_test`, `y_train`, and `y_test`, train an SVC classifier with the default hyper-parameters. What are the accuracy, recall and precision of this classifier on the testing set?\n",
    "\n",
    "*This function should a return a tuple of three floats, i.e. `(accuracy score, recall score, precision score)`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "2b0e40eb06f4274a05049a1bd3ba6f68",
     "grade": false,
     "grade_id": "cell-e6948fe88697bcdf",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score\n",
    "\n",
    "def answer_three():\n",
    "    \n",
    "    model = SVC().fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    acc, rec, pre = (accuracy_score(y_test, y_pred), recall_score(y_test, y_pred), precision_score(y_test, y_pred)) # accuracy, recall and precision\n",
    "\n",
    "    return (acc, rec, pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "cfb20528dd0ae186b1eb84b18e66d62a",
     "grade": true,
     "grade_id": "cell-7dedd52cffa371c6",
     "locked": true,
     "points": 15,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder tests\n",
    "\n",
    "stu_ans = answer_three()\n",
    "\n",
    "assert isinstance(stu_ans, tuple), \"Q3: Your function should return a tuple. \"\n",
    "assert len(stu_ans) == 3, \"Q3: The length of your returned tuple should be 3. \"\n",
    "assert all([isinstance(item, float) for item in stu_ans]), \"Q3: Your tuple should only contain floats. \"\n",
    "\n",
    "del stu_ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "be54370a59a23073e3353dcbe81ea511",
     "grade": false,
     "grade_id": "cell-b1dbd6914b524aca",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Question 4. (20 pts)\n",
    "\n",
    "Train an SVC classifier with default hyper-parameters except for `{\"C\": 1e9, \"gamma\": 1e-8}`. What is the confusion matrix on the testing set if we use a threshold of `-100` for the decision function? That is, we classify instances with a raw score greater than -100 under the decision function as Class 1. \n",
    "\n",
    "*This function should return a 2x2 numpy array of 4 integers.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "fa0c8f3b0b2c0862b2049d274fdc2096",
     "grade": false,
     "grade_id": "cell-9b75dcf2c5c9b4e0",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "def answer_four():\n",
    "  \n",
    "    model = SVC(C = 1e9, gamma = 1e-08).fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = model.decision_function(X_test)\n",
    "    y_pred = np.where(y_pred > -100, 1, 0)\n",
    "    \n",
    "    conf_mtrx = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    return conf_mtrx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "eb2114a2c661e7eb3a2479a5cbf8df10",
     "grade": true,
     "grade_id": "cell-f20e5352013bafae",
     "locked": true,
     "points": 20,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder tests\n",
    "\n",
    "stu_ans = answer_four()\n",
    "\n",
    "assert isinstance(stu_ans, np.ndarray), \"Q4: Your function should return a np.ndarray. \"\n",
    "assert stu_ans.shape == (2, 2), \"Q4: Your confusion matrix should be of size 2x2. \"\n",
    "\n",
    "del stu_ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "ace3130d0057761dfcbd62c348c71a25",
     "grade": false,
     "grade_id": "cell-2b3d6f5fdebbca31",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Question 5. (20 pts)\n",
    "\n",
    "Train a logistic regression spam e-mail classifier with default hyper-parameters using `X_train` and `y_train`. Create a precision-recall curve and a Receiver Operating Characteristic (ROC) curve using `y_test` and the probability estimates of being \"spam\" for X_test.\n",
    "\n",
    "- Based on the precision-recall curve, what is the recall when the precision is $0.90$?\n",
    "\n",
    "- Based on the ROC curve, what is the true positive rate when the false positive rate is $0.10$?\n",
    "\n",
    "Write a function to return the answers. Note you can use a pure programming approach to get the answers: you don't have to actually \"plot\" the curves. However, it's quite instructive to plot these curves to understand just how e.g. precision and recall trade off. So you can plot the curves and read off the answers. Answers correct up to $\\pm 0.02$ are accepted. \n",
    "\n",
    "*This function should return a tuple with two floats, i.e. `(recall, true positive rate)`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "1d6a663463283d099fb84263309178fc",
     "grade": false,
     "grade_id": "cell-c58a02c7d24728be",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "def answer_five():\n",
    "    \n",
    "    model = LogisticRegression().fit(X_train, y_train)\n",
    "    y_scores = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    precision, recall, threshold = precision_recall_curve(y_test, y_scores)\n",
    "    fpr_, tpr_, thresholds = roc_curve(y_test, y_scores)\n",
    "\n",
    "    rec, tpr = recall[np.abs(precision - 0.90).argmin()], tpr_[np.abs(fpr_ - 0.10).argmin()] # recall and TP rate\n",
    "    \n",
    "    return rec, tpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "32e4fd2fd4e5df392eb64e4894ef0a6b",
     "grade": true,
     "grade_id": "cell-baff7afc38159ba6",
     "locked": true,
     "points": 20,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder tests\n",
    "\n",
    "stu_ans = answer_five()\n",
    "\n",
    "assert isinstance(stu_ans, tuple), \"Q5: Your function should return a tuple. \" \n",
    "assert len(stu_ans) == 2, \"Q5: The length of your tuple should be 2. \"\n",
    "assert stu_ans[0] >= 0.7, \"Q5: Your recall value should be greater than 0.7. \"\n",
    "assert stu_ans[1] >= 0.7, \"Q5: Your TP rate value should be greater than 0.7. \"\n",
    "\n",
    "del stu_ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember to comment them out before submitting the notebook\n",
    "\n",
    "# from sklearn.metrics import plot_precision_recall_curve\n",
    "\n",
    "# log_reg = LogisticRegression().fit(X_train, y_train)\n",
    "# disp = plot_precision_recall_curve(log_reg, X_test, y_test)\n",
    "# del log_reg, disp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "9fec0c9c6264bf5810213d6172ede6d2",
     "grade": false,
     "grade_id": "cell-46acd1f5bd1235bb",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Question 6. (15 pts)\n",
    "\n",
    "Perform a grid search over the hyper-parameters listed below for a Logistic Regression classifier, optimizing for classifier **precision** for scoring and five-fold cross validation. \n",
    "\n",
    "**Note: Use the following parameter settings for the logistic regression:**\n",
    " * Use the 'liblinear' solver, which supports both L1 and L2 regularization.\n",
    " * Set `random_state=42`, since the solver uses randomization internally.\n",
    "\n",
    "`'penalty': ['l1', 'l2']`\n",
    "\n",
    "`'C':[0.005, 0.01, 0.05, 0.1, 1, 10]`\n",
    "\n",
    "From `.cv_results_`, create an array of the mean test scores for each hyper-parameter combination. i.e.\n",
    "\n",
    "|   `C`   \t| `l1` \t| `l2` \t|\n",
    "|:----:\t|----\t|----\t|\n",
    "| **`0.005`** \t|    ?\t|   ? \t|\n",
    "| **`0.01`** \t|    ?\t|   ? \t|\n",
    "| **`0.05`** \t|    ?\t|   ? \t|\n",
    "| **`0.1`**  \t|    ?\t|   ? \t|\n",
    "| **`1`**    \t|    ?\t|   ? \t|\n",
    "| **`10`**   \t|    ?\t|   ? \t|\n",
    "\n",
    "<br>\n",
    "\n",
    "*This function should return a 6 by 2 numpy array of floats that contain the values for each \"?\" above. Do not return a pd.DataFrame.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "4843e6a28fa7823dbe1ffc2d4455a33f",
     "grade": false,
     "grade_id": "cell-64775fc70305f296",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def answer_six():    \n",
    "\n",
    "    model = LogisticRegression(random_state = 42, solver = 'liblinear')\n",
    "    \n",
    "    grid_values = {'penalty': ['l1', 'l2'], 'C':[0.005, 0.01, 0.05, 0.1, 1, 10]}\n",
    "    grid = GridSearchCV(model, param_grid = grid_values, scoring = 'precision')\n",
    "    grid.fit(X_train, y_train)\n",
    "\n",
    "    mean_test_scores = grid.cv_results_['mean_test_score'].reshape(6,2)\n",
    "    \n",
    "    return mean_test_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "79c4ba411b83ed1e921d999e9bea9551",
     "grade": true,
     "grade_id": "cell-3ff84c6fb5b851ff",
     "locked": true,
     "points": 15,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder tests\n",
    "\n",
    "stu_ans = answer_six()\n",
    "assert isinstance(stu_ans, np.ndarray), \"Q6: Your function should return a np.ndarray. \"\n",
    "assert stu_ans.shape == (6, 2), \"Q6: Your np.ndarray should be of shape (6, 2). \"\n",
    "\n",
    "del stu_ans "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: use the following function to help visualise the results from the grid search\n",
    "\n",
    "def GridSearch_Heatmap(scores):\n",
    "    import seaborn as sns\n",
    "    import matplotlib.pyplot as plt\n",
    "    %matplotlib inline\n",
    "    \n",
    "    plt.figure()\n",
    "    sns.heatmap(scores.reshape(6, 2), xticklabels=['l1','l2'], yticklabels=[0.005, 0.01, 0.05, 0.1, 1, 10])\n",
    "    plt.yticks(rotation=0);\n",
    "\n",
    "# Remember to comment it out before submitting the notebook\n",
    "# GridSearch_Heatmap(answer_six())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "ad6f4fd340834822e70b4eddddbcfd61",
     "grade": false,
     "grade_id": "cell-ba5820ba9bb669dd",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Question 7. Normalizing features when using regularization (5 pts)\n",
    "\n",
    "Now re-run the code from Question 6 above, but using the raw *unnormalized* training data (i.e. X_train_raw as computed previously as part of Q1.). Return the *highest* precision you obtained from cross-validation in Q6 using normalized features, and the *highest* precision you obtain with the raw, unnormalized features. \n",
    "\n",
    "Your function should return a two-element tuple of floats `(best_precision_normalized, best_precision_unnormalized)`\n",
    "\n",
    "It is very instructive to compare the results from (a) using correctly normalized features, vs. (b) forgetting to normalize the features and (c) the dummy baseline you computed in Q2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "0ecc22b4b95eb32d85d233ab10825529",
     "grade": false,
     "grade_id": "cell-2fb421ea967d0fec",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def answer_seven():  \n",
    "    \n",
    "    model = LogisticRegression(random_state = 42, solver = 'liblinear')\n",
    "    \n",
    "    grid_values = {'penalty': ['l1', 'l2'], 'C':[0.005, 0.01, 0.05, 0.1, 1, 10]}\n",
    "    \n",
    "    grid = GridSearchCV(model, param_grid = grid_values, scoring = 'precision')\n",
    "    grid.fit(X_train, y_train)  \n",
    "    \n",
    "    grid_raw = GridSearchCV(model, param_grid = grid_values, scoring = 'precision')\n",
    "    grid_raw.fit(X_train_raw, y_train)  \n",
    "    \n",
    "    best_precision_normalized = grid.cv_results_['mean_test_score'].max()\n",
    "    best_precision_unnormalized = grid_raw.cv_results_['mean_test_score'].max()\n",
    "    \n",
    "    return (best_precision_normalized, best_precision_unnormalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e7c4ec0de9fadf0d6d82301d48f52460",
     "grade": true,
     "grade_id": "cell-215f1fdf2855ff87",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder tests\n",
    "\n",
    "stu_ans = answer_seven()\n",
    "\n",
    "assert isinstance(stu_ans, tuple), \"Q7: Your function should return a tuple. \"\n",
    "assert len(stu_ans) == 2, \"Q7: The length of your returned tuple should be 2. \"\n",
    "assert all([isinstance(item, float) for item in stu_ans]), \"Q7: Your tuple should only contain floats. \"\n",
    "\n",
    "del stu_ans"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "schema_names": [
    "mads_supervised_learning_v2_assignment3"
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
